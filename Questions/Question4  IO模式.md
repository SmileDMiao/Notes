## 说明
对 32 位操作系统而言，它的寻址空间（虚拟存储空间）为 4G（2 的 32 次方）。
核空间：在 liunx 中，将最高的 1G 字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF），供内核使用，称为 “内核空间”。
用户空间: 在 liunx 中，将较低的 3G 字节（从虚拟地址 0x00000000 到 0xBFFFFFFF），供各个进程使用，称为 “用户空间）。
内核态：当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态
用户态：每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）
因为每个进程可以通过系统调用进入内核，因此，Linux 内核由系统 内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有 4G 字节的虚拟空间。
因为每个进程可以通过系统调用进入内核，因此，Linux 内核由系统 内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有 4G 字节的虚拟空间。

进程寻址空间 0~4G
进程在用户态只能访问 0~3G，只有进入内核态才能访问 3G~4G
进程通过系统调用进入内核态
每个进程虚拟空间的 3G~4G 部分是相同的

为了控制进程的执行，内核必须有能力挂起正在 CPU 上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换（也叫调度）。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的
从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：
保存当前进程 A 的上下文。 上下文就是内核再次唤醒当前进程时所需要的状态，由一些对象（程序计数器、状态寄存器、用户栈等各种内核数据结构）的值组成。
这些值包括描绘地址空间的页表、包含进程相关信息的进程表、文件表等。
切换页全局目录以安装一个新的地址空间。
恢复进程 B 的上下文。
可以理解成一个比较耗资源的过程。

## IO模式
1. 阻塞IO/blocking io
2. 非阻塞IO/noblocking io
3. IO多路复用/io multiplexing(select poll epoll)
4. 信号驱动/signal driven io
5. 异步IO/asynchronous io


![IMAGE](resources/0664A392042ADB0E5D32CD6DD2FA5BF4.jpg =749x396)

![IMAGE](resources/EF6A79BA74534B08DB038EB2C2AC14C3.jpg =777x439)

![IMAGE](resources/065F39469C2DFE909CF1155AE6901364.jpg =811x529)

![IMAGE](resources/A2BC58F39D51E7F1111134C5BFDDF513.jpg =754x507)

![IMAGE](resources/AB3DB44E47DFD2AF30564C503369A038.jpg =709x363)

![IMAGE](resources/F74E6A18169B9BF6257BDB9E7F1CC4D1.jpg =592x338)


## select poll epoll
select的几大缺点:
1. 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
2. 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
3. select支持的文件描述符数量太小了，默认是1024
4. poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，其他的都差不多。


epoll:
1. epoll_create: 创建一个epoll句柄
2. epoll_ctl: 注册要监听的事件类型
3. epoll_wait: 等待事件的产生。
4. select poll非线程安全, epoll线程安全
　

对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。

对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。

select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。

select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。
select和poll都需要在返回后遍历文件描述符来获取已经就绪的socket,描述符越多效率越低,两者异同为

select有最大数量限制,poll没有,但是数量过大都会影响性能(遍历文件描述符)

select使用3个位图表示三种文件描述符,poll使用一个pollfd完成

epoll是select和poll的增强版,epoll通过epoll_ctl()来注册一个文件描述符,一旦基于某个文件描述符就绪时,内核会采用类似callback的回调机制,迅速激活这个文件描述符,当进程调用epoll_wait()时便得到通知(不再需要遍历文件描述符,通过监听回调的机制,也是epoll的魅力)

